<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Text Summarization | CookCv</title><meta name="description" content="什么是NLP中的文本摘要 自动文本摘要是在保持关键信息内容和整体含义的同时，生成简洁流畅的摘要的任务。 文本摘要目前大致可以分为抽取式与生成式两种类型： 1. Extractive Summarization：根据词语重要性、句子重要性排序，抽取出重要度高的句子，从而形成摘要。主要是对文本的选择，算法过程相对更容易，但是对于复杂的文本时，很难仅仅通过选择文本来形成摘要，如小说。 2. Abstra"><meta name="keywords" content="文本摘要,文本总结,注意力机制"><meta name="author" content="Kai Chen"><meta name="copyright" content="Kai Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://cookcv.com/blog/5566fb07.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="Text Summarization"><meta property="og:url" content="http://cookcv.com/blog/5566fb07.html"><meta property="og:site_name" content="CookCv"><meta property="og:description" content="什么是NLP中的文本摘要 自动文本摘要是在保持关键信息内容和整体含义的同时，生成简洁流畅的摘要的任务。 文本摘要目前大致可以分为抽取式与生成式两种类型： 1. Extractive Summarization：根据词语重要性、句子重要性排序，抽取出重要度高的句子，从而形成摘要。主要是对文本的选择，算法过程相对更容易，但是对于复杂的文本时，很难仅仅通过选择文本来形成摘要，如小说。 2. Abstra"><meta property="og:image" content="http://cookcv.com/null"><meta property="article:published_time" content="2020-03-11T17:01:01.000Z"><meta property="article:modified_time" content="2020-03-11T17:01:01.000Z"><meta name="twitter:card" content="summary"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime: '',
  date_suffix: {"one_hour":"刚刚","hours":"小时前","day":"天前"},
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
    },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-03-12 01:01:01'
}</script><noscript><style type="text/css">
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
  }
}

var autoChangeMode = 'false'
var t = saveToLocal.get('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (saveToLocal.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">71</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">92</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/photos/"><i class="fa-fw fa fa-picture-o"></i><span> 照片</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-link"></i><span> 留言板</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFnlp%E4%B8%AD%E7%9A%84%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">什么是NLP中的文本摘要</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#extractive-summarization"><span class="toc-number">1.1.</span> <span class="toc-text">Extractive Summarization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#abstractive-summarization"><span class="toc-number">1.2.</span> <span class="toc-text">Abstractive Summarization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#seq2seq%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">Seq2Seq模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">2.1.</span> <span class="toc-text">Encoder编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">2.2.</span> <span class="toc-text">Decoder解码器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#deocder%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">2.2.1.</span> <span class="toc-text">Deocder的工作流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E9%83%A8%E5%88%86"><span class="toc-number">2.3.</span> <span class="toc-text">推理部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder-decoder%E7%BB%93%E6%9E%84%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">2.4.</span> <span class="toc-text">Encoder-Decoder结构的局限性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">2.5.</span> <span class="toc-text">注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#global-attention-and-local-attention"><span class="toc-number">2.5.1.</span> <span class="toc-text">Global Attention and Local Attention</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%88%98"><span class="toc-number">3.</span> <span class="toc-text">实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%91%98%E8%A6%81%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86"><span class="toc-number">3.1.1.</span> <span class="toc-text">摘要文本处理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83"><span class="toc-number">3.2.</span> <span class="toc-text">数据分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8Btokenizer"><span class="toc-number">3.3.</span> <span class="toc-text">建立Tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B"><span class="toc-number">3.4.</span> <span class="toc-text">模型建立</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">4.</span> <span class="toc-text">数学理解注意力机制</span></a></li></ol></div></div></div><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">CookCv</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/photos/"><i class="fa-fw fa fa-picture-o"></i><span> 照片</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-link"></i><span> 留言板</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Text Summarization</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-03-11T17:01:01.000Z" title="发表于 2020-03-12 01:01:01">2020-03-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-03-11T17:01:01.000Z" title="更新于 2020-03-12 01:01:01">2020-03-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/">自然语言处理NLP</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="什么是nlp中的文本摘要">什么是NLP中的文本摘要</h2>
<p>自动文本摘要是在保持关键信息内容和整体含义的同时，生成简洁流畅的摘要的任务。 文本摘要目前大致可以分为抽取式与生成式两种类型： 1. Extractive Summarization：根据词语重要性、句子重要性排序，抽取出重要度高的句子，从而形成摘要。主要是对文本的选择，算法过程相对更容易，但是对于复杂的文本时，很难仅仅通过选择文本来形成摘要，如小说。 2. Abstractive Summarization：过程更为复杂，但生成能力更强，可认为有一定的概括能力。 <a id="more"></a></p>
<h3 id="extractive-summarization">Extractive Summarization</h3>
<p>由图可以看出，这种方法提取的内容语句来自于原文。 <img src="images/ExtractiveSummarization.jpg" alt="Extractive Summarization" /></p>
<h3 id="abstractive-summarization">Abstractive Summarization</h3>
<p>由图可以看出，这种方法提取的内容语句可能不存在于原文。 <img src="images/AbstractiveSummarization.jpg" alt="Abstractive Summarization" /></p>
<h2 id="seq2seq模型">Seq2Seq模型</h2>
<p>Seq2Seq模型可以处理一切连续型信息，包括情感分类，机器翻译，命名实体识别等。 机器翻译任务中，输入是连续文本序列，输出也是连续文本序列。 命名实体识别中，输入是连续文本序列，输出是连续的标签信息。 所以，我们可以利用Seq2Seq模型，通过输入一段长文本，输出短的摘要，实现文本摘要功能。 下图是典型的Seq2Seq模型架构: <img src="images/Seq2Seq_EncoderDecoder.jpg" alt="Seq2Seq_EncoderDecoder" /> 通常我们可以选择RNNs网络的变体GRU或者LSTM，这是因为它们能够通过克服梯度消失的问题来捕获长期依赖性。</p>
<h3 id="encoder编码器">Encoder编码器</h3>
<p>LSTM中的Encoder读取整个输入序列，其中每个时间step上，都会有一个字输入编码器。然后，他在每个时间step上处理信息，并捕获输入序列中存在的上下文信息。 <img src="images/LSTM_Encoder.jpg" alt="LSTM_Encoder" /> 最后时间step的隐藏层h4与记忆单元层c4将会用来初始化Decoder。</p>
<h3 id="decoder解码器">Decoder解码器</h3>
<p>Decoder是LSTM结构的另一部分。它逐字读取整个目标序列，并以一个时间步长预测相同的序列偏移量。 解码器可以在给定前一个单词的情况下预测序列中的下一个单词。解码器的初始输入是编码器最后一步的结果。 <img src="images/LSTM_Decoder.jpg" alt="LSTM_Decoder" /> 在将整个目标序列放入解码器前，还需将[start] 与 [end]这两个特殊的tokens加入序列中，告知模型的开始与结束。模型通过输入的[start]开始预测第一个词,而[end]则表示整个句子的结束。</p>
<h4 id="deocder的工作流程">Deocder的工作流程</h4>
<p>假设输入序列为[x1,x2,x3,x4],将其编码成内部固定长度的向量。 下图显示了每一个time step下Decoder是如何工作的。 <img src="images/Decoder_Timestep0.jpg" alt="Timestep" /></p>
<h3 id="推理部分">推理部分</h3>
<p>下图是整个Encoder-Decode的结构。通过上面的理解，我觉得这个图非常清晰。 <img src="images/LSTM_Inference.jpg" alt="LSTM_Decoder" /></p>
<ol type="1">
<li>Encoder整个输入序列，并且用Encoder最后一个状态结果来初始化Decoder。</li>
<li>将[start]作为输入传递给解码器Decoder。</li>
<li>使用通过Encoder初始化过的Decoder运行一个time stpe。</li>
<li>输出将是下一个单词的概率，将选择概率最大的单词。</li>
<li>这个预测的单词将会在下一时间Step中作为输入。并且通过当前状态更新内部参数。</li>
<li>重复步骤3-5，直到生成[end]或达到目标序列的最大长度。</li>
</ol>
<h3 id="encoder-decoder结构的局限性">Encoder-Decoder结构的局限性</h3>
<p>Encoder将整个输入序列转为固定的长度，但是当序列很长的时候，Encoder将会很难记住整个序列的内容，无法将所有必要信息准确的编码到固定长度。但是，我们需要关注序列中所有的内容么，不需要。</p>
<h3 id="注意力机制">注意力机制</h3>
<p>为了解决长句子的问题，注意力机制出现在人们的视野。注意力机制为对结果重要的部分添加高的权重，以保留主要信息。举个例子： 1. 需要编码的序列[x1,x2,x3,x4,x5,x6,x7] <strong>Source sequence: Which sport do you like the most?</strong> 2. 需要解码的序列[y1,y2,y3] <strong>Target sequence: I love cricket.</strong></p>
<p>我们可以判断，y1[I]与x4[you]有关，而y2[love]则与x5[like]有关。所以，相比记住序列中的所有单词，不如增加对目标序列重要部分的权重，忽视低权重的部分。</p>
<h4 id="global-attention-and-local-attention">Global Attention and Local Attention</h4>
<p>编码器的隐藏层中，所有部分都参与attention计算上下文。 <img src="images/Sqe2Sqe_GlobalAttention.jpg" alt="全局注意力机制" /> 编码器的隐藏层中，仅有部分参与attention计算上下文。 <img src="images/Sqe2Sqe_GlobalAttention.jpg" alt="局部注意力机制" /></p>
<p>本文最终采用全局注意力机制。（只是添加了注意力机制，编码的固定长度依然需要固定。所以实战中需要通过数据确定一个合适的长度数值。短了无法表达文本内容，长了会造成计算资源浪费。）</p>
<h2 id="实战">实战</h2>
<p>我们的目标是为亚马逊美食评论生成文本摘要。(这里我只提取了我觉得有用的部分) ### 数据表述 这些评论通常很长而且具有可描述性。数据集下载：<a target="_blank" rel="noopener" href="https://www.kaggle.com/snap/amazon-fine-food-reviews">kaggleData</a>。 数据涵盖了超过10年的时间，包括截至2012年10月的所有〜500,000条评论。这些评论包括产品，用户信息，评级，纯文本评论和摘要。它还包括来自所有其他亚马逊类别的评论。</p>
<h3 id="数据处理">数据处理</h3>
<p>由于评论文本和摘要中涉及的预处理步骤略有不同，因此我们需要定义两个不同的函数来预处理评论和摘要。 #### 评论文本处理 1. 将所有字母小写； 2. 移除HTML标签； 3. Contraction mapping； 4. 移除(‘s)； 5. 删除括号内的内容(觉得括号里面的内容解释说明不重要)； 6. 消除标点符号和特殊字符； 7. 删除停用词； 8. 删除低频词；</p>
<h4 id="摘要文本处理">摘要文本处理</h4>
<p>为摘要文本添加[start]和[end]。</p>
<h3 id="数据分布">数据分布</h3>
<p>通过数据统计，可以看到摘要与文本数据的长度分布。通过数据可视化，我们可以将评论文本的长度限定在80，而摘要的长度限定在10。 <img src="images/SummayText_Length.jpg" alt="SummayText Length" /></p>
<h3 id="建立tokenizer">建立Tokenizer</h3>
<p>通过分词器生成词汇表，并将单词文本序列转为数值序列，方便计算机计算。</p>
<h3 id="模型建立">模型建立</h3>
<ol type="1">
<li>我们可以选择是否让LSTM在每个时间步都会生成隐藏状态h和记忆单元状态c。</li>
<li>选择LSTM是否仅生成最后一个时间步的隐藏状态h和记忆单元状态c。</li>
<li>选择LSTM相互堆叠提高模型效果。</li>
<li>选择双向LSTM，可以双向处理文本数据，获取更加丰富的上下文信息。</li>
<li>使用beam search strategy代替贪婪方法argmax。</li>
<li>根据BLEU分数评估模型的性能。</li>
<li>可以选择指针生成网络，</li>
<li>因为整数序列采用独热编码的方式，所以损失函数采用了稀疏交叉熵，对内存友好。</li>
</ol>
<h2 id="数学理解注意力机制">数学理解注意力机制</h2>
<ol type="1">
<li>编码器为源文本序列每一个时间步j都生成了一个隐藏状态值hj。</li>
<li>相似的工作，解码器为目标文本每一个时间步i都生成了隐藏状态值si。</li>
<li><p>alignment score: <span class="math inline">\(e_{ij}\)</span>。用这个分数表示源文本中的第j步单词与目标文本中第i步单词的关联度。可以用hj与si来计算这个分数值<span class="math inline">\(e_{ij} = score(s_i,h_j)\)</span> 根据所使用的得分函数的类型，有不同类型的注意力机制。这里列举一些流行的注意力机制： <img src="images/LSTM_Attention_ScoreFunction.jpg" alt="ScoreFunction" /></p></li>
<li>使用softmax函数对注意力参数的值进行归一化。<span class="math inline">\(a_{ij}=\frac{e_{ij}}{\sum^{T}_{k=1}e_{ik}}\)</span></li>
<li>计算注意力权重<span class="math inline">\(a_{ij}\)</span>与编码器hj的隐藏状态乘积的线性总和，以产生注意力上下文向量Ci。<span class="math inline">\(C_{i} = \sum^T_{j=1}a_{ij}h_{ij}\)</span> <img src="images/LSTM_Attention_C1.jpg" alt="Attention_C1" /></li>
<li>将注意力上一下文向量Ci与目标隐藏层向量si级联以产生新的注意力隐藏层向量Si。<span class="math inline">\(S_i=concatenate([s_{i};C_{i}])\)</span></li>
<li><p>将注意力隐藏层向量传入密集层产生yi。<span class="math inline">\(y_{i}=dense(S_{i})\)</span></p></li>
</ol>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/?utm_source=blog&amp;utm_medium=understanding-transformers-nlp-state-of-the-art-models">英文原文</a> 本文由公众号【深度学习视觉】整理。</p>
</blockquote>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Kai Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://cookcv.com/blog/5566fb07.html">http://cookcv.com/blog/5566fb07.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://cookcv.com" target="_blank">CookCv</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Text-Summarization/">Text Summarization</a><a class="post-meta__tags" href="/tags/Attention/">Attention</a><a class="post-meta__tags" href="/tags/LSTM/">LSTM</a></div><div class="post_share"><div class="social-share" data-image="/null" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/blog/4f7ea913.html"><img class="next-cover" src="/null" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CNN如何学到图片中的位置信息</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Kai Chen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>